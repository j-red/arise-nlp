{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_ Preprocesses the csv file created using sc_warts2csv scamper tool \n",
    "_ Create csv files containing RTT values\n",
    "\n",
    "INPUT: csv file created as a result of converting warts file to csv file\n",
    "OUTPUT: csv files containing RTT values, each file for one source-destination (SD) pair\n",
    "\n",
    "USAGE\n",
    "Specify the target directory to store resulting csv files\n",
    "Specify fname, min_num_rtt, max_num_rtt\n",
    "  fname: the csv file resulting from using sc_warts2csv scamper tool (users need to provide their own)\n",
    "  min_num_rtt: the minimum number of data points for each dataset (current value: 2000)\n",
    "  max_num_rtt: the maximum number of data points for each dataset (current value: 4000)\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import os, sys, glob\n",
    "sys.path.append('../')\n",
    "from utils import get_full_path\n",
    "\n",
    "\n",
    "# Import csv file\n",
    "# Specify path to csv file\n",
    "print(\"... Import csv file\")\n",
    "\n",
    "fname = get_full_path('atl2-20190101.csv') # this file is not provided, users need to supply their own\n",
    "\n",
    "# Data was obtained from http://data.caida.org/datasets/topology/ark/ipv4/probe-data/team-1/2019/cycle-20190101/\n",
    "# Dataset used was the atl2-us.team-probing.c007143.20190101.warts dataset, converted to CSV using sc_warts2csv\n",
    "\n",
    "df = pd.read_csv(fname, header=None, sep=';')\n",
    "\n",
    "# Set header to version, userID, timestamp, ...\n",
    "print(\"... Set header to version, userID, timestamp, ...\")\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.columns = new_header\n",
    "\n",
    "# Remove unnecessary lines\n",
    "print(\"... Remove unnecessary lines\")\n",
    "df_new = df[df['version'] != 'version']\n",
    "\n",
    "# Extract only hopaddrs, timestamp, and rtts\n",
    "print(\"... Extract only hopaddrs, timestamp, and rtts\")\n",
    "df_hopaddr_timestamp_rtt = df_new[['timestamp','hopaddr', 'rtt']]\n",
    "df_hopaddr_timestamp_rtt['timestamp_rtt'] = df[['timestamp', 'rtt']].apply(tuple, axis=1)\n",
    "\n",
    "df_final = df_hopaddr_timestamp_rtt.set_index('hopaddr')\n",
    "\n",
    "# Group by hopaddrs with rtts in list\n",
    "print(\"... Group by hopaddrs with rtts in list\")\n",
    "df_hrt = df_final['timestamp_rtt']\n",
    "df_hrt_grouped = df_hrt.groupby(df_hrt.index).apply(list)\n",
    "df_hrt_final = df_hrt_grouped.reset_index()\n",
    "\n",
    "# Extract only the hopaddrs with the number of rtt more than desired value\n",
    "print(\"... Extract only the hopaddrs with the number of rtt more than desired value\")\n",
    "min_num_rtt = 2000\n",
    "max_num_rtt = 4000\n",
    "df_hrt_final2 = df_hrt_final[df_hrt_final.timestamp_rtt.apply(lambda x: len(x) > min_num_rtt and len(x) < max_num_rtt)]\n",
    "\n",
    "# Create csv files containing the RTTs, one csv file for each client, sort by datetime\n",
    "print(\"... Create csv files containing the RTTs, one csv file for each client\")\n",
    "\n",
    "num_results = df_hrt_final2.shape[0] \n",
    "for i in range(num_results):\n",
    "    hop = df_hrt_final2['hopaddr'].iloc[i]\n",
    "\n",
    "    num_tuples = len(df_hrt_final2['timestamp_rtt'].iloc[i])    \n",
    "    rtts = []\n",
    "    timestamps = []\n",
    "    for j in range(num_tuples):\n",
    "        timestamp = df_hrt_final2['timestamp_rtt'].iloc[i][j][0]\n",
    "        rtt = df_hrt_final2['timestamp_rtt'].iloc[i][j][1]\n",
    "        rtts.append(rtt)\n",
    "        timestamps.append(timestamp)\n",
    "    timestamp_rtt = {'timestamp': timestamps, 'rtt': rtts}\n",
    "    # create new dataframe from these lists\n",
    "    df_file = pd.DataFrame.from_dict(timestamp_rtt)\n",
    "    # convert epoch to datetime\n",
    "    df_file['datetime'] = pd.to_datetime(df_file['timestamp'],unit='s')\n",
    "    # sort by datetime\n",
    "    df_file_sorted = df_file.sort_values(by='datetime')\n",
    "    # only take columns datetime and rtt\n",
    "    df_file_dt_rtt = df_file_sorted[['datetime', 'rtt']]\n",
    "    # reset index\n",
    "    df_file_final = df_file_dt_rtt.reset_index(drop=True)\n",
    "    \n",
    "    # save to csv    \n",
    "    # print(str(hop) + '.csv')\n",
    "    df_file_final.to_csv(str(hop) + '.csv')\n",
    "\n",
    "\n",
    "# rename filenames \n",
    "files = glob.glob('*.csv')\n",
    "for i in range(len(files)):\n",
    "    os.rename(files[i], f'dataset_{i:02d}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emerge_env",
   "language": "python",
   "name": "emerge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
